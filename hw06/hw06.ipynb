{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #6: Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DUE:__ Monday 2/26 11:59am\n",
    "\n",
    "__HOW TO SUBMIT:__ __We are moving to a new submission system that is powered by git__ We are no longer going to use [WebSubmit](https://www.cs.duke.edu/csed/websubmit/). We have created a SUBMIT repository for almost every one on [coursework gitlab](https://coursework.cs.duke.edu). Make sure you have access to CS216-s2018-stu/your_firstname-your_lastname-SUBMIT; otherwise please contact zjmiao@cs.duke.edu as soon as possible. \n",
    "\n",
    "_If this is the first time you are submitting a hw or lab assignment:_\n",
    "\n",
    "Fire up your VM, open a terminal, type in the following commands to initialize your SUBMIT repository:\n",
    "```shell\n",
    "cd ~\n",
    "git clone git@coursework.cs.duke.edu:CS216-s2018-stu/<Firstname>-<Lastname>-SUBMIT.git SUBMIT\n",
    "cd SUBMIT\n",
    "touch README.md\n",
    "git add README.md\n",
    "git commit -m \"add README\"\n",
    "git push -u origin master\n",
    "```\n",
    "Replace `<Firstname>` and `<Lastname>` with your name as it appears on the coursework gitlab page. If this is successful, you should see a new folder SUBMIT in your home directory. \n",
    "\n",
    "_How to submit an assignment:_ \n",
    "\n",
    "Create a new folder `hw05` directly inside your SUBMIT directory. \n",
    "```shell\n",
    "cd ~/SUBMIT\n",
    "mkdir hw06\n",
    "```\n",
    "\n",
    "And copy all required files (from your local `working` directory, see __WHAT TO SUBMIT__ under each problem below) into it. The sample folder structure should look like this:\n",
    "```shell\n",
    "/SUBMIT\n",
    "/SUBMIT/hw06\n",
    "/SUBMIT/hw06/hw06-q1.ipynb\n",
    "/SUBMIT/hw06/hw06-q2.ipynb\n",
    "...\n",
    "```    \n",
    "\n",
    "Finally, when your are ready to submit your homework, run the commands below:\n",
    "```shell\n",
    "cd ~/SUBMIT\n",
    "git add hw06\n",
    "git commit -m \"updated hw06\"\n",
    "git push\n",
    "```\n",
    "You can replace \"updated hw06\" with any meaningful message you want.\n",
    "\n",
    "If your submission is successful, you will see your files in your repository on gitlab website.\n",
    "\n",
    "You can submit multiple times, but __we will only grade the files under /hw06 directory__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WHAT TO SUBMIT:__ Nothing is required for this part.\n",
    "\n",
    "To get ready for this assignment, open up a VM shell, and type the following command:\n",
    "\n",
    "```shell\n",
    "~/CS216-s2018-READONLY/sync.sh\n",
    "```\n",
    "\n",
    "Next, type the following commands to create a working directory for this homework. Here we use `hw06` under your `working/` directory, but feel free to change it to another location.\n",
    "\n",
    "```shell\n",
    "cd ~\n",
    "cp -r ~/CS216-s2018-READONLY/assignments/hw06 ~/working/\n",
    "cd ~/working/hw06\n",
    "```\n",
    "\n",
    "We will be working with the same products dataset introducted in Lab #3, focusing on the Amazon products this time.  You can use your choice of programming language for this homework (e.g., SQL or python).  If you choose to use SQL, you can access the database using: \n",
    "\n",
    "    psql products\n",
    "\n",
    "which should have been set up in Lab #3.  If not, you can also run `./setup.sh` (from Lab 3) to recreate the database.\n",
    "\n",
    "If you plan to use Python for this homework, you can parse the file `amazon.ascii.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Finding Salient Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going find salient words (appearing in the title, description or manufacturer) for each product in the amazon dataset. You will need to implement:  \n",
    "\n",
    "__(A)__ a method for tokenizing strings to get words, and\n",
    "\n",
    "__(B)__ a method for computing TFIDF scores for the words. \n",
    "  \n",
    "You can choose to implement any method for tokenizing strings (split on spaces, punctuation, etc). We hope to use the results of this homework in Lab #6, and a good tokenizer might help you do better in the lab exercise.\n",
    "\n",
    "_SQL HINTS:_ If you are using SQL, you will need to use string functions which can be found <a href=\"http://www.postgresql.org/docs/9.3/static/functions-string.html\">here</a>.  You can use `regexp_split_to_table` to split a string based on a regular expression.  For example:\n",
    "\n",
    "    SELECT id, regexp_split_to_table(title, '\\s+') AS word FROM amazon;\n",
    "\n",
    "will split the `title` string of each product in `amazon` table into substrings by whitespace, and output one row for each substring.  (You will need to tweak this query to get better tokenization.)\n",
    "\n",
    "Also, you might want to create a view with schema `word, product_id, tfidfscore`, which can then be used in the next part.\n",
    "\n",
    "_Python HINTS:_ If you choose to use Python, you may get the data in two ways:\n",
    "(1) directly parse the data in amazon.ascii.csv using python. You can see examples in: \n",
    "* `lab05/regression.py` (which uses pandas package's read_csv command) or\n",
    "* `hw05/reviews/prepare.py` (which uses the with open command)\n",
    "(2) connect to the products database using the command `conn =\n",
    "psycopg2.connect(\"dbname=products\")` as we did in the `congress/prepare.py` in `hw06`, and then use SQL queries to get the data.\n",
    "\n",
    "In Python you may use the `re.split` command to split a string based on a regular expression to get an array of substrings (described [here](https://docs.python.org/2/library/re.html)).\n",
    "\n",
    "__WHAT TO SUBMIT:__ Create a jupyter notebook `hw06-q1.ipynb` (you can make a copy of existing any existing notebook from previous lectures). Include your code:\n",
    "* If SQL: you can include the query in a Markdown cell. \n",
    "* If Python: make a code cell and include your code there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcm/.local/lib/python2.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname=products\")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nCREATE VIEW amazon_stat (product_id, word, tfidfscore) AS\n\tWITH amzwords(product_id, word) AS (\t\n\t\tSELECT id,\n\t\t\tregexp_split_to_table(title||' '||description||' '||manufacturer, E'\\s+')\n\t\t\tAS word\n\t\tFROM amazon),\n\n-- IDF = number of documents word appears in\n\nidfs(word, idf) AS (\n\tSELECT docw.word, COUNT(*)\n\tFROM (SELECT DISTINCT product_id, word FROM amzwords) AS docw\n\tGROUP BY word), \n\n-- TF = number of times word appears in a document \n\ntfs(product_id, word, tf) AS (\n\t\tSELECT product_id, word, COUNT(*) FROM amzwords\n\t\tGROUP BY product_id, word), \n\ntotalnumproducts AS (SELECT COUNT(DISTINCT product_id) AS \n\tnumids FROM amzwords)\n\n-- compute saliency \n\nSELECT tf.product_id, tf.word, tf.tf*log(totalnumproducts.numids/idf.idf) AS tfidf\n\tFROM tfs AS tf, idfs AS idf, totalnumproducts\n\tWHERE tf.word = idf.word\n\tORDER BY tf.product_id, tf.tf*log(totalnumproducts.numids/idf.idf) DESC;\n\nSELECT word, tfidfscore FROM amazon_data;\n': relation \"amazon_stat\" already exists\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-941112d31e12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \"\"\"\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/vcm/.local/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             chunksize=chunksize)\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vcm/.local/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vcm/.local/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1419\u001b[0m             ex = DatabaseError(\n\u001b[1;32m   1420\u001b[0m                 \"Execution failed on sql '%s': %s\" % (args[0], exc))\n\u001b[0;32m-> 1421\u001b[0;31m             \u001b[0mraise_with_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vcm/.local/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql '\nCREATE VIEW amazon_stat (product_id, word, tfidfscore) AS\n\tWITH amzwords(product_id, word) AS (\t\n\t\tSELECT id,\n\t\t\tregexp_split_to_table(title||' '||description||' '||manufacturer, E'\\s+')\n\t\t\tAS word\n\t\tFROM amazon),\n\n-- IDF = number of documents word appears in\n\nidfs(word, idf) AS (\n\tSELECT docw.word, COUNT(*)\n\tFROM (SELECT DISTINCT product_id, word FROM amzwords) AS docw\n\tGROUP BY word), \n\n-- TF = number of times word appears in a document \n\ntfs(product_id, word, tf) AS (\n\t\tSELECT product_id, word, COUNT(*) FROM amzwords\n\t\tGROUP BY product_id, word), \n\ntotalnumproducts AS (SELECT COUNT(DISTINCT product_id) AS \n\tnumids FROM amzwords)\n\n-- compute saliency \n\nSELECT tf.product_id, tf.word, tf.tf*log(totalnumproducts.numids/idf.idf) AS tfidf\n\tFROM tfs AS tf, idfs AS idf, totalnumproducts\n\tWHERE tf.word = idf.word\n\tORDER BY tf.product_id, tf.tf*log(totalnumproducts.numids/idf.idf) DESC;\n\nSELECT word, tfidfscore FROM amazon_data;\n': relation \"amazon_stat\" already exists\n"
     ]
    }
   ],
   "source": [
    "# fill in the sql query for part 1 --> creates view that part 2 will use \n",
    "q1 = \"\"\"\n",
    "CREATE VIEW amazon_stat (product_id, word, tfidfscore) AS\n",
    "\tWITH amzwords(product_id, word) AS (\t\n",
    "\t\tSELECT id,\n",
    "\t\t\tregexp_split_to_table(title||' '||description||' '||manufacturer, E'\\\\s+')\n",
    "\t\t\tAS word\n",
    "\t\tFROM amazon),\n",
    "\n",
    "-- IDF = number of documents word appears in\n",
    "\n",
    "idfs(word, idf) AS (\n",
    "\tSELECT docw.word, COUNT(*)\n",
    "\tFROM (SELECT DISTINCT product_id, word FROM amzwords) AS docw\n",
    "\tGROUP BY word), \n",
    "\n",
    "-- TF = number of times word appears in a document \n",
    "\n",
    "tfs(product_id, word, tf) AS (\n",
    "\t\tSELECT product_id, word, COUNT(*) FROM amzwords\n",
    "\t\tGROUP BY product_id, word), \n",
    "\n",
    "totalnumproducts AS (SELECT COUNT(DISTINCT product_id) AS \n",
    "\tnumids FROM amzwords)\n",
    "\n",
    "-- compute saliency \n",
    "\n",
    "SELECT tf.product_id, tf.word, tf.tf*log(totalnumproducts.numids/idf.idf) AS tfidf\n",
    "\tFROM tfs AS tf, idfs AS idf, totalnumproducts\n",
    "\tWHERE tf.word = idf.word\n",
    "\tORDER BY tf.product_id, tf.tf*log(totalnumproducts.numids/idf.idf) DESC;\n",
    "\n",
    "SELECT word, tfidfscore FROM amazon_data;\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(q1, con=conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Finding Similar Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have quantified the saliency of words for each product, implement an algorithm that given as input an Amazon product id, outputs the top 5 Amazon products in the database that are most similar to the input product.\n",
    "\n",
    "For example, given product id `1931102953` as input, you might see (besides `1931102953` itself) `b000bgpqos` and `b000ivhozk` in the top 5 list.\n",
    "\n",
    "\n",
    "__HINT:__ Use cosine similarity discussed in class. Here is an example output when the input id is `1931102953`. Note the similarity scores you get can be different based on how you tokenize the words. \n",
    "\n",
    "```\n",
    "     id     |      cosine      \n",
    "------------+------------------\n",
    " 1931102953 | 9.63630192360657\n",
    " b000bgpqos | 3.87480547182896\n",
    " b000ivhozk | 2.28476732325212\n",
    " b000eg6d5q | 2.10493603888233\n",
    " b0002dlrru | 1.98766834498716\n",
    "```\n",
    "\n",
    "__WHAT TO SUBMIT:__ Create a jupyter notebook `hw06-q2.ipynb` (you can make a copy of existing any existing notebook from previous lectures). Include your code for computing the top 5 product ids that are most similar to an input product id.\n",
    "* If SQL: you can include the query in a Markdown cell, and a sample output (for one input id) in another Markdown cell.  \n",
    "* If Python: make a code cell and include your code there. Run the code for one input id. \n",
    "* _DO NOT USE THE SAME INPUT ID AS SHOWN IN THE SAMPLE OUTPUT IN THE HOMEWORK_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>cossimilarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1931102953</td>\n",
       "      <td>9.454836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b000bgpqos</td>\n",
       "      <td>4.601199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b000ivhozk</td>\n",
       "      <td>2.768284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b000eg6d5q</td>\n",
       "      <td>2.585601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b0002dlrru</td>\n",
       "      <td>2.547438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  cossimilarity\n",
       "0  1931102953       9.454836\n",
       "1  b000bgpqos       4.601199\n",
       "2  b000ivhozk       2.768284\n",
       "3  b000eg6d5q       2.585601\n",
       "4  b0002dlrru       2.547438"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill in the sql query for part 2\n",
    "q2 = \"\"\"\n",
    "-- d(D1, D2) = sum (w*y) / ((sqrt sum w^2) * (sqrt sum y^2))\n",
    "\n",
    "SELECT y.product_id, sum(w.tfidfscore*y.tfidfscore/sqrt(botw.dim * boty.dim)) \n",
    "\tAS cossimilarity\n",
    "FROM amazon_stat w, amazon_stat y, \n",
    "\t(SELECT product_id, sqrt(sum(tfidfscore*tfidfscore)) AS dim\n",
    "\t\tFROM amazon_stat GROUP BY product_id) AS botw, \n",
    "\t(SELECT product_id, sqrt(sum(tfidfscore*tfidfscore)) AS dim\n",
    "\t\tFROM amazon_stat GROUP BY product_id) AS boty\n",
    "WHERE w.product_id = '1931102953' AND w.word = y.word AND \n",
    "\tw.product_id = botw.product_id AND y.product_id = boty.product_id\n",
    "GROUP BY y.product_id\n",
    "ORDER BY cossimilarity DESC limit 5; \n",
    "\n",
    "\"\"\"\n",
    "pd.read_sql(q2, con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to close the connection to the database when you are done.\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Project Teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit your project teams via a text file `proj-teams.txt` in a folder named `proj-teams`. Only one person in each team needs to submit this. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
