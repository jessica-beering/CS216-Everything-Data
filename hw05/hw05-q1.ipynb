{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #5: Classification\n",
    "\n",
    "## 1. Laptop vs. Phone "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have collected over 5000 reviews of products that are either cell phones or laptops. In this exercise we are going to write your own Naive Bayes classifier for deciphering whether a review pertains to a Laptop or a Phone.\n",
    "\n",
    "In the `reviews/` subdirectory, you will find a (poorly formatted) CSV file named `reviews.csv`.  First, you need to run the Python script `prepare.py` (in the first code cell below), which parses these file, constructs the objects (e.g., feature matrix and outcome vector) to be used by your classifier, and writes them into a `.pik` for faster loading by your subsequent analysis.\n",
    "\n",
    "For a description of what these objects mean, see comments in the code cells below.\n",
    "\n",
    "Your job is to modify and complete this notebook file to implement Naive Bayes classification according to the specification given in the code.  You need to implement the algorithm by yourself; do __NOT__ use the Naive Bayes implementations provided by Python `sklearn` package.\n",
    "\n",
    "In addition to accuracy, note that the program will also print out the most important features used by this classifier---assuming that you have implemented the algorithm correctly according to the specification.  This information can be very useful debugging.  Do your top features make intuitive sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcm/.local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY\n",
    "import sys\n",
    "import numpy\n",
    "from sklearn import cross_validation\n",
    "from sklearn.base import BaseEstimator\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the following cell to explore the datastructures that are used to store the features $(X)$ and labels $(Y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5648 instances\n",
      "29427 features\n",
      "Review # B00JJ9687W.json\n",
      "Feature # 3238 = after\n",
      "Label for Review # B00JJ9687W.json = 0\n",
      "Does the feature `after` appear in B00JJ9687W.json? yes\n",
      "The number of words that appear in this review: 141\n",
      "All features for this review as a *dense* array [0 0 0 ... 0 0 0] ... this has 141 1's\n",
      "All features for this review as a *sparse* array (i.e., it only stores the indexes of features that are non-zero) \n",
      "  (0, 95)\t1\n",
      "  (0, 3238)\t1\n",
      "  (0, 3250)\t1\n",
      "  (0, 3479)\t1\n",
      "  (0, 3525)\t1\n",
      "  (0, 3609)\t1\n",
      "  (0, 3687)\t1\n",
      "  (0, 3733)\t1\n",
      "  (0, 3745)\t1\n",
      "  (0, 3842)\t1\n",
      "  (0, 3878)\t1\n",
      "  (0, 4156)\t1\n",
      "  (0, 4161)\t1\n",
      "  (0, 4737)\t1\n",
      "  (0, 4785)\t1\n",
      "  (0, 5321)\t1\n",
      "  (0, 5393)\t1\n",
      "  (0, 5544)\t1\n",
      "  (0, 5728)\t1\n",
      "  (0, 5761)\t1\n",
      "  (0, 5906)\t1\n",
      "  (0, 5934)\t1\n",
      "  (0, 5971)\t1\n",
      "  (0, 7684)\t1\n",
      "  (0, 8038)\t1\n",
      "  :\t:\n",
      "  (0, 26335)\t1\n",
      "  (0, 26424)\t1\n",
      "  (0, 26460)\t1\n",
      "  (0, 26499)\t1\n",
      "  (0, 26566)\t1\n",
      "  (0, 26846)\t1\n",
      "  (0, 26931)\t1\n",
      "  (0, 26933)\t1\n",
      "  (0, 26982)\t1\n",
      "  (0, 27555)\t1\n",
      "  (0, 27578)\t1\n",
      "  (0, 27585)\t1\n",
      "  (0, 27591)\t1\n",
      "  (0, 27597)\t1\n",
      "  (0, 27702)\t1\n",
      "  (0, 27898)\t1\n",
      "  (0, 28336)\t1\n",
      "  (0, 28388)\t1\n",
      "  (0, 28584)\t1\n",
      "  (0, 28629)\t1\n",
      "  (0, 28766)\t1\n",
      "  (0, 28836)\t1\n",
      "  (0, 28927)\t1\n",
      "  (0, 28947)\t1\n",
      "  (0, 28996)\t1\n"
     ]
    }
   ],
   "source": [
    "# This is how the reviews data is preprocessed into feature vectors (X) and labels (Y)\n",
    "X, y, instances, features = prepare.get_data()\n",
    "\n",
    "# instances: a list (array) of review ids, one for each review in reviews.csv.\n",
    "# features: a list (array) containing words that ever appear somewhere in reviews.\n",
    "print '{} instances'.format(X.shape[0])\n",
    "print '{} features'.format(X.shape[1])\n",
    "\n",
    "print 'Review # {}'.format(instances[0]) # you can replace 0 with a number between 0 and X.shape[0]-1\n",
    "print 'Feature # 3238 = {}'.format(features[3238]) # you can replace 0 with a number between 0 and X.shape[1]-1\n",
    "\n",
    "\n",
    "# y: a vector (array) with one component per instance;\n",
    "#    y[i] = 0 if review with id instance[i] is for a laptop,\n",
    "#        or 1 if it is for a mobile phone.\n",
    "print 'Label for Review # {} = {}'.format(instances[0], y[0]) # you can replace 0 with a number between 0 and X.shape[0]-1\n",
    "\n",
    "# X: a matrix with one row per instance and one column per feature;n\n",
    "#    X[i,j] = 1 if word features[j] appears in review with id instance[i],\n",
    "#          or 0 if the word doesn't appear in it.\n",
    "\n",
    "# X[i, j] tells you the value of feature j for instance i\n",
    "print 'Does the feature `{}` appear in {}? {}'.format(features[3238], instances[0], 'yes' if X[0, 3238]==1 else 'no')\n",
    "\n",
    "print 'The number of words that appear in this review: {}'.format(X[0, :].getnnz())\n",
    "\n",
    "print 'All features for this review as a *dense* array {} ... this has {} 1\\'s'.format(X[0].toarray()[0], sum(X[0].toarray()[0]))\n",
    "print 'All features for this review as a *sparse* array (i.e., it only stores the indexes of features that are non-zero) \\n{}'.format(X[0,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to edit the code in the following cells to create your own Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "#\n",
    "# Please follow the comments inside the following coding cells and implement what it asks\n",
    "# \n",
    "# This section is the definition of the class MyNaiveBayes\n",
    "# The actual implementations of class functions are defined in the following three cells.\n",
    "#\n",
    "\n",
    "class MyNaiveBayes(BaseEstimator):\n",
    "    def __init__(self): pass\n",
    "    def fit(self, X_train, y_train): pass\n",
    "    def predict(self, X_test): pass\n",
    "    def score(self, X_test, y_test):\n",
    "        return float(sum(predicted == actual \\\n",
    "                         for predicted, actual \\\n",
    "                         in zip(self.predict(X_test), y_test))) \\\n",
    "            / len(y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY IF REQUIRED\n",
    "\n",
    "def __init__(self):\n",
    "    # You should set the following attributes in the fit() method.\n",
    "    # class_log_prior_:\n",
    "    #   an array of two floats (real numbers), where class_log_prior_[k] \n",
    "    #   is the natural logarithm of the probability (\n",
    "    #   estimated from training data) that\n",
    "    #   class is k (k is 0 or 1).\n",
    "    # feature_count_:\n",
    "    #   number of features, or the number of columns in X.\n",
    "    # feature_log_prob_:\n",
    "    #   a matrix of floats, with two rows and as many columns as\n",
    "    #   self.feature_count_; feature_log_prob_[k,f] is the natural logarithm\n",
    "    #   of the probability (estimated from training data) of seeing the word\n",
    "    #   corresponding to feature f as part of a review of class k. zip(x_train,y_train)\n",
    "    self.class_log_prior_ = None\n",
    "    self.feature_count_ = None\n",
    "    self.feature_log_prob_ = None\n",
    "    # You may use additional attributes as needed.\n",
    "    \n",
    "MyNaiveBayes.__init__ = __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY AND COMPLETE\n",
    "\n",
    "def fit(self, X_train, y_train): #x-train 0 and 1 \n",
    "    # Train your classifier with given data.  See comments below\n",
    "    # on X and y for an explanation of the format of X_train,\n",
    "    # y_train (labels of phone or laptop)\n",
    "    #\n",
    "    # REPLACE THE FOLLOWING (INCORRECT) WITH YOUR IMPLEMENTATION:\n",
    "    self.class_log_prior_ = numpy.log([(y_train.sum()*1.0/len(y_train)*1.0), ((len(y_train) - y_train.sum()*1.0)/len(y_train)*1.0)])  #([, 0.5]) #Num reviews in class(laptop or phone) / total num reviews\n",
    "    self.feature_count_ = X_train.shape[1]\n",
    "    self.feature_log_prob_ = numpy.zeros((2, self.feature_count_))\n",
    "    \n",
    "    counts = numpy.zeros((2, self.feature_count_)) #creates array \n",
    "    \n",
    "    for review,k in zip(X_train, y_train): #([0,1,1] 1/0) k =0 or 1 laptop or phone\n",
    "        counts[k] += review.toarray()[0]\n",
    "        \n",
    "        #count[k].sum()\n",
    "        \n",
    "        #in counts we have the total number of occurances of each word in all of reviews combined \n",
    "        \n",
    "        #get probability of word appearing in reviews overall \n",
    "        \n",
    "            # num words (in counts) +1 / (# of words in laptop reviews + total number of words)\n",
    "    \n",
    "    n_instances = X_train.shape[0]*1.0 #number of laptop reviews\n",
    "    n_observations = X_train.sum()*1.0 #number of words (in all reviews)\n",
    "    \n",
    "    for k in (0, 1):\n",
    "        for f in range(self.feature_count_): #for every feature \n",
    "            self.feature_log_prob_[k,f] = numpy.log((counts[k][f]*1.0 + 1) / ((counts[k].sum()*1.0) + n_observations)) #numpy.log(1.0 / n_observations) \n",
    "\n",
    "MyNaiveBayes.fit = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY AND COMPLETE\n",
    "\n",
    "def predict(self, X_test):\n",
    "    # Return a vector (array) y_test; each component holds the\n",
    "    # predicted class for each row of X_test.  See comments below\n",
    "    # on X and y for an explanation of the format of X_test,\n",
    "    # y_test.\n",
    "    #\n",
    "    # As a simplification, you may assume that all words in the\n",
    "    # testing data are already in your vocabulary (so there is a\n",
    "    # feature for each); however, do not assume that every word\n",
    "    # appears in the training data.\n",
    "    #\n",
    "    # REPLACE THE FOLLOWING (INCORRECT) WITH YOUR IMPLEMENTATION:\n",
    "    y_test = list()\n",
    "    \n",
    "    for x in X_test: # for each row\n",
    "        probphone = 0.0\n",
    "        problaptop = 0.0\n",
    "       \n",
    "        x = x.toarray()[0]\n",
    "        for i in range(0,len(x)):\n",
    "            if x[i] == 1:\n",
    "                problaptop += self.feature_log_prob_[0,i]\n",
    "                probphone += self.feature_log_prob_[1,i]\n",
    "                \n",
    "        problaptop += self.class_log_prior_[0]\n",
    "        probphone += self.class_log_prior_[1]\n",
    "            \n",
    "        if problaptop > probphone:\n",
    "            y_test.append(0)\n",
    "        else: \n",
    "            y_test.append(1)\n",
    "            \n",
    "            \n",
    "       # if x.sum() % 2 == 0:\n",
    "            #y_test.append(1)\n",
    "       # else:\n",
    "         #   y_test.append(0) for f in range(self.feature_count_):\n",
    "        \n",
    "        \n",
    "    return numpy.array(y_test)\n",
    "\n",
    "MyNaiveBayes.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5648 instances, 29427 features\n",
      "======================================================================\n",
      "debugging on one train/test split:\n",
      "train accuracy: 0.8198\n",
      "test accuracy: 0.7511\n",
      "top 20 features:\n",
      "\t             texting: -5.3085\t                asus: +5.0305\n",
      "\t               nokia: -5.1918\t                  i7: +4.8827\n",
      "\t                 htc: -5.0377\t               intel: +4.6383\n",
      "\t            unlocked: -4.8784\t                 ssd: +4.5559\n",
      "\t            motorola: -4.8487\t                 hdd: +4.5497\n",
      "\t                 gsm: -4.7275\t                  i3: +4.4593\n",
      "\t              sprint: -4.4697\t                bios: +4.1983\n",
      "\t       international: -4.4598\t             toshiba: +4.1717\n",
      "\t                  fm: -4.3986\t                  i5: +4.1443\n",
      "\t                 lte: -4.3986\t                acer: +4.0471\n",
      "======================================================================\n",
      "running 10-fold cross validation...\n",
      "[0.06902655 0.06017699 0.04247788 0.55575221 1.         1.\n",
      " 1.         1.         1.         1.        ]\n",
      "accuracy: 0.6727 (+/- 0.8469)\n"
     ]
    }
   ],
   "source": [
    "# After you have implemented and runned the cells above, excute this cell to get the output of the classification algorithm\n",
    "%run -i classify_main.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
