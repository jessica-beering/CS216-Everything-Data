{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #10: MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DUE:__ 11:59am April 18th\n",
    "\n",
    "__HOW TO SUBMIT:__ __We are moving to a new submission system that is powered by git__ \n",
    "\n",
    "We are no longer going to use [WebSubmit](https://www.cs.duke.edu/csed/websubmit/). We have created a SUBMIT repository for almost every one on [coursework gitlab](https://coursework.cs.duke.edu). Make sure you have access to CS216-s2018-stu/your_firstname-your_lastname-SUBMIT; otherwise please contact zjmiao@cs.duke.edu as soon as possible. \n",
    "\n",
    "_If this is the first time you are submitting a hw or lab assignment:_\n",
    "\n",
    "Fire up your VM, open a terminal, type in the following commands to initialize your SUBMIT repository:\n",
    "```shell\n",
    "cd ~\n",
    "git clone git@coursework.cs.duke.edu:CS216-s2018-stu/<Firstname>-<Lastname>-SUBMIT.git SUBMIT\n",
    "cd SUBMIT\n",
    "touch README.md\n",
    "git add README.md\n",
    "git commit -m \"add README\"\n",
    "git push -u origin master\n",
    "```\n",
    "Replace `<Firstname>` and `<Lastname>` with your name as it appears on the coursework gitlab page. If this is successful, you should see a new folder SUBMIT in your home directory. \n",
    "\n",
    "_How to submit an assignment:_ \n",
    "\n",
    "Create a new folder `hw10` directly inside your SUBMIT directory. \n",
    "```shell\n",
    "cd ~/SUBMIT\n",
    "mkdir hw10\n",
    "```\n",
    "\n",
    "And copy all required files (from your local `working` directory, see __WHAT TO SUBMIT__ under each problem below) into it. The sample folder structure should look like this:\n",
    "```shell\n",
    "/SUBMIT\n",
    "/SUBMIT/hw10\n",
    "/SUBMIT/hw10/hw10.ipynb\n",
    "```    \n",
    "\n",
    "Finally, when your are ready to submit your homework, run the commands below:\n",
    "```shell\n",
    "cd ~/SUBMIT\n",
    "git add hw10\n",
    "git commit -m \"updated hw10\"\n",
    "git push\n",
    "```\n",
    "You can replace \"updated hw10\" with any meaningful message you want.\n",
    "\n",
    "If your submission is successful, you will see your files in your repository on gitlab website.\n",
    "\n",
    "You can submit multiple times, but __we will only grade the files under /hw10 directory in your LATEST submission__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#     0. Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WHAT TO SUBMIT:__ Nothing is required for this part.\n",
    "\n",
    "To get ready for this assignment, open up a VM shell, and type the following command:\n",
    "\n",
    "```shell\n",
    "cd ~/CS216-s2018-READONLY/\n",
    "git pull\n",
    "```\n",
    "\n",
    "Next, type the following commands to create a working directory for this homework. Here we use `hw10` under your `shared/` directory, but feel free to change it to another location.\n",
    "\n",
    "```shell\n",
    "cd ~\n",
    "cp -r ~/CS216-s2018-READONLY/assignments/hw10 ~/working/\n",
    "cd ~/working/hw10\n",
    "```\n",
    "    \n",
    "Next, run the following commands to **install and set up Spark environment**.\n",
    "\n",
    "```shell\n",
    "./installSpark.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Finding the top 5 Twitter hashtags locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will write a MapReduce program in Python to find the 5 most popular hashtags from (1) a small file containing approximately 1,000 tweets. (2) a medium file containing approximately 100,000 tweets. We strongly recommend that you debug first running on the small file before running on the medium file. You can run the program on both input files locally.\n",
    "\n",
    "**Hint**: Your approach should work on much bigger datasets since your code should also work for large dataset in part 2. Keep in mind the tips from class about potential problems when computing on data in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PySpark` Library and an Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major Python package that you'll use is `PySpark`, which allows people to program Spark with Python. You're highly recommended to read its official programming guide (https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds) to know how to use its APIs. And we have provided an example program `example_word_count.py`, which takes in a text file, counts the number of times that each word occurs in the file, and outputs the five most frequent words in it. You could open the file to read it in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell installs the Python `pyspark` package first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/vcm/.local/lib/python2.7/site-packages\r\n",
      "Requirement already satisfied: py4j==0.10.6 in /home/vcm/.local/lib/python2.7/site-packages (from pyspark)\r\n"
     ]
    }
   ],
   "source": [
    "!pip --no-cache-dir install pyspark --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell runs the example word count program on the file `tweets_sm.txt`. Right now you don't need to know what the command is exactly doing, just run it to get a sense of how it works. From the output you can see the 5 most frequent words and their counts in the given file are:\n",
    "\n",
    "```\n",
    "a 82\n",
    "I'm 73\n",
    "I 71\n",
    "to 69\n",
    "the 68\n",
    "```\n",
    "\n",
    "The outputs of Spark include its running logs, possible error messages, and real outputs of our program. By specifying `2>log` in the following command, only `stdout` will be printed in the notebook, and running logs will be redirected to the file called `log`. If you encounter an error, you can go and check the log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --master local[4] example_word_count_local.py tweets_sm.txt 2>log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a Python Spark Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The usual way to run a Python Spark program locally with shell commands:\n",
    "\n",
    "Assume your Python Spark program is stored in a file called `hashtag_count.py`.\n",
    "\n",
    "In the shell, type the following commands:\n",
    "\n",
    "```shell\n",
    "cd ~/working/hw10\n",
    "spark-submit --master local[4] hashtag_count.py tweets_sm.txt\n",
    "spark-submit --master local[4] hashtag_count.py tweets_med.txt\n",
    "```\n",
    "\n",
    "The first command is to go to the working directory where all your program files (.py) and input files (.txt) exist.\n",
    "\n",
    "The second command is to run the Spark program on the small input. \n",
    "\n",
    "The third command is to run the Spark program on the medium input. \n",
    "\n",
    "4 is number of cores. You can modify it to suit your needs.\n",
    "\n",
    "* In our homework, we have put the Python code and the shell `spark-submit` command into the notebook. You can just follow the notebook instructions to modify the Python code in the notebook, and then run the Spark application in the notebook too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT TO SUBMIT**: `hw10.ipynb` notebook where you've modified the actual code cell, and run the two `spark-submit` cells and kept their outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Actual Coding Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the first line `%%writefile hashtag_count.py`, everytime you run the following cell, its content will be written to a file called `hashtag_count.py`. If the file name already exists, its contents will be overwritten silently. So DON'T modify the first line in the cell otherwise you won't be able to save code to file. \n",
    "\n",
    "We will need to run the `hashtag_count.py` file for the Spark application. So please make sure the output file (i.e., the code in the following cell) has valid Python file syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hashtag_count.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hashtag_count.py\n",
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import heapq\n",
    "import operator\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: topK_local <file>\", file=sys.stderr)\n",
    "        exit(-1)\n",
    "\n",
    "    # Initialize the spark context.\n",
    "    sc = SparkContext(\"local\", \"PythonHashTag\")\n",
    "\n",
    "    # You can modify partition number according to your need\n",
    "    numPartitions = 3\n",
    "\n",
    "    # make a new RDD from the input text\n",
    "    lines = sc.textFile(sys.argv[1], numPartitions)\n",
    "\n",
    "    # [Map stage] split lines to words\n",
    "    words = lines.flatMap(lambda lines: lines.split(' '))\n",
    "    # lambda x: f(x) ... means for every x run the function f(x)\n",
    "\n",
    "    # [Map stage] filter words to only get hashtags\n",
    "    # FIX THIS LINE\n",
    "    hashtags = words.filter(lambda word: len(word) > 0 and word[0]=='#')\n",
    "\n",
    "    # [Map stage] turn word to (word, 1) keyvalue pair\n",
    "    hashtagsNum = hashtags.map(lambda word: (word, 1))\n",
    "\n",
    "    # [Reduce stage] add up values for each distinct key\n",
    "    hashtagsCount = hashtagsNum.reduceByKey(add)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # You can have multiple MapReduce stages.\n",
    "    # You will find mapPartitions() helpful\n",
    "    \n",
    "    reversedhashtagsCount = hashtagsCount.map(lambda (hashtag, count): (count, hashtag))\n",
    "    sortedReversedhashtagsCount = reversedhashtagsCount.sortByKey(ascending=False)\n",
    "    sortedhashtagsCount = sortedReversedhashtagsCount.map(lambda (count, hashtag): (hashtag, count))\n",
    "    \n",
    "    for (hashtag, count) in sortedhashtagsCount.take(5):\n",
    "        print(hashtag, count)\n",
    "\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell executes a shell command:\n",
    "```shell\n",
    "spark-submit --master local[4] hashtag_count.py tweets_sm.txt\n",
    "```\n",
    "inside Jupyter notebook.\n",
    "\n",
    "This shell command is to run the Python code locally in Spark, with `tweets_sm.txt` as the input to the Python program. 4 is number of cores and you can modify it to suit your needs. You'll see its output printed in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#OmSpikTanya 6\r\n",
      "#NP 2\r\n",
      "#20cosesulmiocompagnodibanco 2\r\n",
      "#ReMix 2\r\n",
      "#CheerUp 1\r\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --master local[4] hashtag_count.py tweets_sm.txt 2>log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the following cell is to run the same Python program in Spark, but with `tweets_med.txt` as input to the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#OmSpikTanya 140\r\n",
      "# 137\r\n",
      "#Team 97\r\n",
      "#GGMU 76\r\n",
      "#TeamFollowBack 51\r\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --master local[4] hashtag_count.py tweets_med.txt 2>log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Finding the top 50 Twitter hashtags on cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is optional. You will get extra credit for this part.\n",
    "\n",
    "In this part, you will run your code in Part1 on cloud to find the **50** most popular hashtags from a large file containing approximately 3.5 million tweets (a couple hours in the twitterverse).\n",
    "\n",
    "The small tweets file (for testing) is stored at: **s3://cs216-spring2015/twitter/tweets_sm.txt**\n",
    "\n",
    "The large tweets file is stored at: **s3://cs216-spring2015/twitter/tweets-full/* **\n",
    "\n",
    "We strongly recommend that you debug first locally before running on the large dataset on cloud. \n",
    "\n",
    "You can make a copy of the your local code:\n",
    "\n",
    "```shell\n",
    "cp hashtag_count.py hashtag_count_aws.py\n",
    "```\n",
    "\n",
    "and edit `hashtag_count_aws.py` to make it work on cloud.\n",
    "\n",
    "To run the Spark program on cloud, please study the instructions in `~/working/hw10/spark-tutorial-aws.pdf`. \n",
    "\n",
    "To know how to modify the local program, how to use S3 files, please study the example file `example_word_count_aws.py`. You could copy some configuration code from `example_word_count_aws.py` to your `hashtag_count_aws.py`, but please remember to change `MASTER_ADDRESS`, `AWS ACCESS KEY` and `AWS SECRET ACCESS KEY`.\n",
    "    \n",
    "**WHAT TO SUBMIT**: `hw10.ipynb` with your answers pasted in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste your answers (50 top hashtags) in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
